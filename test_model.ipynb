{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhomnhom0/miniforge3/envs/face_recognition/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EdgeFace_KANLinear(\n",
      "  (downsample_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): LayerNorm()\n",
      "      (1): Conv2d(48, 96, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): LayerNorm()\n",
      "      (1): Conv2d(96, 160, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): LayerNorm()\n",
      "      (1): Conv2d(160, 304, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (stages): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.006)\n",
      "      )\n",
      "      (2): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.012)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.018)\n",
      "      )\n",
      "      (1): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.024)\n",
      "      )\n",
      "      (2): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.029)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.035)\n",
      "      )\n",
      "      (1): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.041)\n",
      "      )\n",
      "      (2): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.047)\n",
      "      )\n",
      "      (3): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.053)\n",
      "      )\n",
      "      (4): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.059)\n",
      "      )\n",
      "      (5): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.065)\n",
      "      )\n",
      "      (6): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.071)\n",
      "      )\n",
      "      (7): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.076)\n",
      "      )\n",
      "      (8): ConvEncoder_KANLinear(\n",
      "        (dwconv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.082)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): SDTAEncoder_KANLinear(\n",
      "        (convs): ModuleList(\n",
      "          (0-3): 4 x Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=61)\n",
      "        )\n",
      "        (norm_xca): LayerNorm()\n",
      "        (xca): XCA_KANLinear(\n",
      "          (qkv): LoRaLin_KAN(\n",
      "            (linear1): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "            (linear2): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): LoRaLin_KAN(\n",
      "            (linear1): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "            (linear2): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.088)\n",
      "      )\n",
      "      (1): SDTAEncoder_KANLinear(\n",
      "        (convs): ModuleList(\n",
      "          (0-3): 4 x Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=61)\n",
      "        )\n",
      "        (norm_xca): LayerNorm()\n",
      "        (xca): XCA_KANLinear(\n",
      "          (qkv): LoRaLin_KAN(\n",
      "            (linear1): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "            (linear2): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): LoRaLin_KAN(\n",
      "            (linear1): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "            (linear2): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.094)\n",
      "      )\n",
      "      (2): SDTAEncoder_KANLinear(\n",
      "        (convs): ModuleList(\n",
      "          (0-3): 4 x Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=61)\n",
      "        )\n",
      "        (norm_xca): LayerNorm()\n",
      "        (xca): XCA_KANLinear(\n",
      "          (qkv): LoRaLin_KAN(\n",
      "            (linear1): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "            (linear2): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): LoRaLin_KAN(\n",
      "            (linear1): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "            (linear2): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin_KAN(\n",
      "          (linear1): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "          (linear2): KANLinear(\n",
      "            (base_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.100)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((304,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): LoRaLin_KAN(\n",
      "    (linear1): KANLinear(\n",
      "      (base_activation): SiLU()\n",
      "    )\n",
      "    (linear2): KANLinear(\n",
      "      (base_activation): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (head_dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "from models.KANLinear_EdgeFace import EdgeFace_KANLinear\n",
    "\n",
    "# Define the model\n",
    "model = EdgeFace_KANLinear(\n",
    "    in_chans=3, \n",
    "    num_classes=512,\n",
    "    depths=[3, 3, 9, 3],\n",
    "    dims=[48, 96, 160, 304],\n",
    "    global_block=[0, 0, 0, 3],\n",
    "    global_block_type=['None', 'None', 'None', 'SDTA'],\n",
    "    drop_path_rate=0.1, \n",
    "    layer_scale_init_value=1e-6, \n",
    "    head_init_scale=1.0, \n",
    "    expan_ratio=4,\n",
    "    kernel_sizes=[7, 7, 7, 7],\n",
    "    heads=[8, 8, 8, 8],\n",
    "    use_pos_embd_xca=[False, False, False, False],\n",
    "    use_pos_embd_global=False,\n",
    "    d2_scales=[2, 3, 4, 5],\n",
    "    classifier_dropout=0.5  # Add any additional kwargs\n",
    ")\n",
    "\n",
    "# Print the model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor of shape (batch_size, 3, 112, 112) to pass through the model\n",
    "input_tensor = torch.randn(1, 3, 112, 112)  # Batch size of 1\n",
    "\n",
    "# Pass the tensor through the model\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Print the output tensor shape\n",
    "print(f\"Output tensor shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 48, 28, 28]           2,352\n",
      "         LayerNorm-2           [-1, 48, 28, 28]              96\n",
      "            Conv2d-3           [-1, 48, 28, 28]           2,400\n",
      "         LayerNorm-4           [-1, 28, 28, 48]              96\n",
      "              SiLU-5                   [-1, 48]               0\n",
      "         KANLinear-6           [-1, 28, 28, 24]               0\n",
      "              SiLU-7                   [-1, 24]               0\n",
      "         KANLinear-8          [-1, 28, 28, 192]               0\n",
      "       LoRaLin_KAN-9          [-1, 28, 28, 192]               0\n",
      "             GELU-10          [-1, 28, 28, 192]               0\n",
      "             SiLU-11                  [-1, 192]               0\n",
      "        KANLinear-12           [-1, 28, 28, 24]               0\n",
      "             SiLU-13                   [-1, 24]               0\n",
      "        KANLinear-14           [-1, 28, 28, 48]               0\n",
      "      LoRaLin_KAN-15           [-1, 28, 28, 48]               0\n",
      "         Identity-16           [-1, 48, 28, 28]               0\n",
      "ConvEncoder_KANLinear-17           [-1, 48, 28, 28]               0\n",
      "           Conv2d-18           [-1, 48, 28, 28]           2,400\n",
      "        LayerNorm-19           [-1, 28, 28, 48]              96\n",
      "             SiLU-20                   [-1, 48]               0\n",
      "        KANLinear-21           [-1, 28, 28, 24]               0\n",
      "             SiLU-22                   [-1, 24]               0\n",
      "        KANLinear-23          [-1, 28, 28, 192]               0\n",
      "      LoRaLin_KAN-24          [-1, 28, 28, 192]               0\n",
      "             GELU-25          [-1, 28, 28, 192]               0\n",
      "             SiLU-26                  [-1, 192]               0\n",
      "        KANLinear-27           [-1, 28, 28, 24]               0\n",
      "             SiLU-28                   [-1, 24]               0\n",
      "        KANLinear-29           [-1, 28, 28, 48]               0\n",
      "      LoRaLin_KAN-30           [-1, 28, 28, 48]               0\n",
      "         DropPath-31           [-1, 48, 28, 28]               0\n",
      "ConvEncoder_KANLinear-32           [-1, 48, 28, 28]               0\n",
      "           Conv2d-33           [-1, 48, 28, 28]           2,400\n",
      "        LayerNorm-34           [-1, 28, 28, 48]              96\n",
      "             SiLU-35                   [-1, 48]               0\n",
      "        KANLinear-36           [-1, 28, 28, 24]               0\n",
      "             SiLU-37                   [-1, 24]               0\n",
      "        KANLinear-38          [-1, 28, 28, 192]               0\n",
      "      LoRaLin_KAN-39          [-1, 28, 28, 192]               0\n",
      "             GELU-40          [-1, 28, 28, 192]               0\n",
      "             SiLU-41                  [-1, 192]               0\n",
      "        KANLinear-42           [-1, 28, 28, 24]               0\n",
      "             SiLU-43                   [-1, 24]               0\n",
      "        KANLinear-44           [-1, 28, 28, 48]               0\n",
      "      LoRaLin_KAN-45           [-1, 28, 28, 48]               0\n",
      "         DropPath-46           [-1, 48, 28, 28]               0\n",
      "ConvEncoder_KANLinear-47           [-1, 48, 28, 28]               0\n",
      "        LayerNorm-48           [-1, 48, 28, 28]              96\n",
      "           Conv2d-49           [-1, 96, 14, 14]          18,528\n",
      "           Conv2d-50           [-1, 96, 14, 14]           4,800\n",
      "        LayerNorm-51           [-1, 14, 14, 96]             192\n",
      "             SiLU-52                   [-1, 96]               0\n",
      "        KANLinear-53           [-1, 14, 14, 48]               0\n",
      "             SiLU-54                   [-1, 48]               0\n",
      "        KANLinear-55          [-1, 14, 14, 384]               0\n",
      "      LoRaLin_KAN-56          [-1, 14, 14, 384]               0\n",
      "             GELU-57          [-1, 14, 14, 384]               0\n",
      "             SiLU-58                  [-1, 384]               0\n",
      "        KANLinear-59           [-1, 14, 14, 48]               0\n",
      "             SiLU-60                   [-1, 48]               0\n",
      "        KANLinear-61           [-1, 14, 14, 96]               0\n",
      "      LoRaLin_KAN-62           [-1, 14, 14, 96]               0\n",
      "         DropPath-63           [-1, 96, 14, 14]               0\n",
      "ConvEncoder_KANLinear-64           [-1, 96, 14, 14]               0\n",
      "           Conv2d-65           [-1, 96, 14, 14]           4,800\n",
      "        LayerNorm-66           [-1, 14, 14, 96]             192\n",
      "             SiLU-67                   [-1, 96]               0\n",
      "        KANLinear-68           [-1, 14, 14, 48]               0\n",
      "             SiLU-69                   [-1, 48]               0\n",
      "        KANLinear-70          [-1, 14, 14, 384]               0\n",
      "      LoRaLin_KAN-71          [-1, 14, 14, 384]               0\n",
      "             GELU-72          [-1, 14, 14, 384]               0\n",
      "             SiLU-73                  [-1, 384]               0\n",
      "        KANLinear-74           [-1, 14, 14, 48]               0\n",
      "             SiLU-75                   [-1, 48]               0\n",
      "        KANLinear-76           [-1, 14, 14, 96]               0\n",
      "      LoRaLin_KAN-77           [-1, 14, 14, 96]               0\n",
      "         DropPath-78           [-1, 96, 14, 14]               0\n",
      "ConvEncoder_KANLinear-79           [-1, 96, 14, 14]               0\n",
      "           Conv2d-80           [-1, 96, 14, 14]           4,800\n",
      "        LayerNorm-81           [-1, 14, 14, 96]             192\n",
      "             SiLU-82                   [-1, 96]               0\n",
      "        KANLinear-83           [-1, 14, 14, 48]               0\n",
      "             SiLU-84                   [-1, 48]               0\n",
      "        KANLinear-85          [-1, 14, 14, 384]               0\n",
      "      LoRaLin_KAN-86          [-1, 14, 14, 384]               0\n",
      "             GELU-87          [-1, 14, 14, 384]               0\n",
      "             SiLU-88                  [-1, 384]               0\n",
      "        KANLinear-89           [-1, 14, 14, 48]               0\n",
      "             SiLU-90                   [-1, 48]               0\n",
      "        KANLinear-91           [-1, 14, 14, 96]               0\n",
      "      LoRaLin_KAN-92           [-1, 14, 14, 96]               0\n",
      "         DropPath-93           [-1, 96, 14, 14]               0\n",
      "ConvEncoder_KANLinear-94           [-1, 96, 14, 14]               0\n",
      "        LayerNorm-95           [-1, 96, 14, 14]             192\n",
      "           Conv2d-96            [-1, 160, 7, 7]          61,600\n",
      "           Conv2d-97            [-1, 160, 7, 7]           8,000\n",
      "        LayerNorm-98            [-1, 7, 7, 160]             320\n",
      "             SiLU-99                  [-1, 160]               0\n",
      "       KANLinear-100             [-1, 7, 7, 80]               0\n",
      "            SiLU-101                   [-1, 80]               0\n",
      "       KANLinear-102            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-103            [-1, 7, 7, 640]               0\n",
      "            GELU-104            [-1, 7, 7, 640]               0\n",
      "            SiLU-105                  [-1, 640]               0\n",
      "       KANLinear-106             [-1, 7, 7, 80]               0\n",
      "            SiLU-107                   [-1, 80]               0\n",
      "       KANLinear-108            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-109            [-1, 7, 7, 160]               0\n",
      "        DropPath-110            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-111            [-1, 160, 7, 7]               0\n",
      "          Conv2d-112            [-1, 160, 7, 7]           8,000\n",
      "       LayerNorm-113            [-1, 7, 7, 160]             320\n",
      "            SiLU-114                  [-1, 160]               0\n",
      "       KANLinear-115             [-1, 7, 7, 80]               0\n",
      "            SiLU-116                   [-1, 80]               0\n",
      "       KANLinear-117            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-118            [-1, 7, 7, 640]               0\n",
      "            GELU-119            [-1, 7, 7, 640]               0\n",
      "            SiLU-120                  [-1, 640]               0\n",
      "       KANLinear-121             [-1, 7, 7, 80]               0\n",
      "            SiLU-122                   [-1, 80]               0\n",
      "       KANLinear-123            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-124            [-1, 7, 7, 160]               0\n",
      "        DropPath-125            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-126            [-1, 160, 7, 7]               0\n",
      "          Conv2d-127            [-1, 160, 7, 7]           8,000\n",
      "       LayerNorm-128            [-1, 7, 7, 160]             320\n",
      "            SiLU-129                  [-1, 160]               0\n",
      "       KANLinear-130             [-1, 7, 7, 80]               0\n",
      "            SiLU-131                   [-1, 80]               0\n",
      "       KANLinear-132            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-133            [-1, 7, 7, 640]               0\n",
      "            GELU-134            [-1, 7, 7, 640]               0\n",
      "            SiLU-135                  [-1, 640]               0\n",
      "       KANLinear-136             [-1, 7, 7, 80]               0\n",
      "            SiLU-137                   [-1, 80]               0\n",
      "       KANLinear-138            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-139            [-1, 7, 7, 160]               0\n",
      "        DropPath-140            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-141            [-1, 160, 7, 7]               0\n",
      "          Conv2d-142            [-1, 160, 7, 7]           8,000\n",
      "       LayerNorm-143            [-1, 7, 7, 160]             320\n",
      "            SiLU-144                  [-1, 160]               0\n",
      "       KANLinear-145             [-1, 7, 7, 80]               0\n",
      "            SiLU-146                   [-1, 80]               0\n",
      "       KANLinear-147            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-148            [-1, 7, 7, 640]               0\n",
      "            GELU-149            [-1, 7, 7, 640]               0\n",
      "            SiLU-150                  [-1, 640]               0\n",
      "       KANLinear-151             [-1, 7, 7, 80]               0\n",
      "            SiLU-152                   [-1, 80]               0\n",
      "       KANLinear-153            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-154            [-1, 7, 7, 160]               0\n",
      "        DropPath-155            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-156            [-1, 160, 7, 7]               0\n",
      "          Conv2d-157            [-1, 160, 7, 7]           8,000\n",
      "       LayerNorm-158            [-1, 7, 7, 160]             320\n",
      "            SiLU-159                  [-1, 160]               0\n",
      "       KANLinear-160             [-1, 7, 7, 80]               0\n",
      "            SiLU-161                   [-1, 80]               0\n",
      "       KANLinear-162            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-163            [-1, 7, 7, 640]               0\n",
      "            GELU-164            [-1, 7, 7, 640]               0\n",
      "            SiLU-165                  [-1, 640]               0\n",
      "       KANLinear-166             [-1, 7, 7, 80]               0\n",
      "            SiLU-167                   [-1, 80]               0\n",
      "       KANLinear-168            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-169            [-1, 7, 7, 160]               0\n",
      "        DropPath-170            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-171            [-1, 160, 7, 7]               0\n",
      "          Conv2d-172            [-1, 160, 7, 7]           8,000\n",
      "       LayerNorm-173            [-1, 7, 7, 160]             320\n",
      "            SiLU-174                  [-1, 160]               0\n",
      "       KANLinear-175             [-1, 7, 7, 80]               0\n",
      "            SiLU-176                   [-1, 80]               0\n",
      "       KANLinear-177            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-178            [-1, 7, 7, 640]               0\n",
      "            GELU-179            [-1, 7, 7, 640]               0\n",
      "            SiLU-180                  [-1, 640]               0\n",
      "       KANLinear-181             [-1, 7, 7, 80]               0\n",
      "            SiLU-182                   [-1, 80]               0\n",
      "       KANLinear-183            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-184            [-1, 7, 7, 160]               0\n",
      "        DropPath-185            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-186            [-1, 160, 7, 7]               0\n",
      "          Conv2d-187            [-1, 160, 7, 7]           8,000\n",
      "       LayerNorm-188            [-1, 7, 7, 160]             320\n",
      "            SiLU-189                  [-1, 160]               0\n",
      "       KANLinear-190             [-1, 7, 7, 80]               0\n",
      "            SiLU-191                   [-1, 80]               0\n",
      "       KANLinear-192            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-193            [-1, 7, 7, 640]               0\n",
      "            GELU-194            [-1, 7, 7, 640]               0\n",
      "            SiLU-195                  [-1, 640]               0\n",
      "       KANLinear-196             [-1, 7, 7, 80]               0\n",
      "            SiLU-197                   [-1, 80]               0\n",
      "       KANLinear-198            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-199            [-1, 7, 7, 160]               0\n",
      "        DropPath-200            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-201            [-1, 160, 7, 7]               0\n",
      "          Conv2d-202            [-1, 160, 7, 7]           8,000\n",
      "       LayerNorm-203            [-1, 7, 7, 160]             320\n",
      "            SiLU-204                  [-1, 160]               0\n",
      "       KANLinear-205             [-1, 7, 7, 80]               0\n",
      "            SiLU-206                   [-1, 80]               0\n",
      "       KANLinear-207            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-208            [-1, 7, 7, 640]               0\n",
      "            GELU-209            [-1, 7, 7, 640]               0\n",
      "            SiLU-210                  [-1, 640]               0\n",
      "       KANLinear-211             [-1, 7, 7, 80]               0\n",
      "            SiLU-212                   [-1, 80]               0\n",
      "       KANLinear-213            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-214            [-1, 7, 7, 160]               0\n",
      "        DropPath-215            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-216            [-1, 160, 7, 7]               0\n",
      "          Conv2d-217            [-1, 160, 7, 7]           8,000\n",
      "       LayerNorm-218            [-1, 7, 7, 160]             320\n",
      "            SiLU-219                  [-1, 160]               0\n",
      "       KANLinear-220             [-1, 7, 7, 80]               0\n",
      "            SiLU-221                   [-1, 80]               0\n",
      "       KANLinear-222            [-1, 7, 7, 640]               0\n",
      "     LoRaLin_KAN-223            [-1, 7, 7, 640]               0\n",
      "            GELU-224            [-1, 7, 7, 640]               0\n",
      "            SiLU-225                  [-1, 640]               0\n",
      "       KANLinear-226             [-1, 7, 7, 80]               0\n",
      "            SiLU-227                   [-1, 80]               0\n",
      "       KANLinear-228            [-1, 7, 7, 160]               0\n",
      "     LoRaLin_KAN-229            [-1, 7, 7, 160]               0\n",
      "        DropPath-230            [-1, 160, 7, 7]               0\n",
      "ConvEncoder_KANLinear-231            [-1, 160, 7, 7]               0\n",
      "       LayerNorm-232            [-1, 160, 7, 7]             320\n",
      "          Conv2d-233            [-1, 304, 3, 3]         194,864\n",
      "          Conv2d-234             [-1, 61, 3, 3]             610\n",
      "          Conv2d-235             [-1, 61, 3, 3]             610\n",
      "          Conv2d-236             [-1, 61, 3, 3]             610\n",
      "          Conv2d-237             [-1, 61, 3, 3]             610\n",
      "       LayerNorm-238               [-1, 9, 304]             608\n",
      "            SiLU-239                  [-1, 304]               0\n",
      "       KANLinear-240               [-1, 9, 152]               0\n",
      "            SiLU-241                  [-1, 152]               0\n",
      "       KANLinear-242               [-1, 9, 912]               0\n",
      "     LoRaLin_KAN-243               [-1, 9, 912]               0\n",
      "         Dropout-244            [-1, 8, 38, 38]               0\n",
      "            SiLU-245                  [-1, 304]               0\n",
      "       KANLinear-246               [-1, 9, 152]               0\n",
      "            SiLU-247                  [-1, 152]               0\n",
      "       KANLinear-248               [-1, 9, 304]               0\n",
      "     LoRaLin_KAN-249               [-1, 9, 304]               0\n",
      "         Dropout-250               [-1, 9, 304]               0\n",
      "   XCA_KANLinear-251               [-1, 9, 304]               0\n",
      "        DropPath-252               [-1, 9, 304]               0\n",
      "       LayerNorm-253            [-1, 3, 3, 304]             608\n",
      "            SiLU-254                  [-1, 304]               0\n",
      "       KANLinear-255            [-1, 3, 3, 152]               0\n",
      "            SiLU-256                  [-1, 152]               0\n",
      "       KANLinear-257           [-1, 3, 3, 1216]               0\n",
      "     LoRaLin_KAN-258           [-1, 3, 3, 1216]               0\n",
      "            GELU-259           [-1, 3, 3, 1216]               0\n",
      "            SiLU-260                 [-1, 1216]               0\n",
      "       KANLinear-261            [-1, 3, 3, 152]               0\n",
      "            SiLU-262                  [-1, 152]               0\n",
      "       KANLinear-263            [-1, 3, 3, 304]               0\n",
      "     LoRaLin_KAN-264            [-1, 3, 3, 304]               0\n",
      "        DropPath-265            [-1, 304, 3, 3]               0\n",
      "SDTAEncoder_KANLinear-266            [-1, 304, 3, 3]               0\n",
      "          Conv2d-267             [-1, 61, 3, 3]             610\n",
      "          Conv2d-268             [-1, 61, 3, 3]             610\n",
      "          Conv2d-269             [-1, 61, 3, 3]             610\n",
      "          Conv2d-270             [-1, 61, 3, 3]             610\n",
      "       LayerNorm-271               [-1, 9, 304]             608\n",
      "            SiLU-272                  [-1, 304]               0\n",
      "       KANLinear-273               [-1, 9, 152]               0\n",
      "            SiLU-274                  [-1, 152]               0\n",
      "       KANLinear-275               [-1, 9, 912]               0\n",
      "     LoRaLin_KAN-276               [-1, 9, 912]               0\n",
      "         Dropout-277            [-1, 8, 38, 38]               0\n",
      "            SiLU-278                  [-1, 304]               0\n",
      "       KANLinear-279               [-1, 9, 152]               0\n",
      "            SiLU-280                  [-1, 152]               0\n",
      "       KANLinear-281               [-1, 9, 304]               0\n",
      "     LoRaLin_KAN-282               [-1, 9, 304]               0\n",
      "         Dropout-283               [-1, 9, 304]               0\n",
      "   XCA_KANLinear-284               [-1, 9, 304]               0\n",
      "        DropPath-285               [-1, 9, 304]               0\n",
      "       LayerNorm-286            [-1, 3, 3, 304]             608\n",
      "            SiLU-287                  [-1, 304]               0\n",
      "       KANLinear-288            [-1, 3, 3, 152]               0\n",
      "            SiLU-289                  [-1, 152]               0\n",
      "       KANLinear-290           [-1, 3, 3, 1216]               0\n",
      "     LoRaLin_KAN-291           [-1, 3, 3, 1216]               0\n",
      "            GELU-292           [-1, 3, 3, 1216]               0\n",
      "            SiLU-293                 [-1, 1216]               0\n",
      "       KANLinear-294            [-1, 3, 3, 152]               0\n",
      "            SiLU-295                  [-1, 152]               0\n",
      "       KANLinear-296            [-1, 3, 3, 304]               0\n",
      "     LoRaLin_KAN-297            [-1, 3, 3, 304]               0\n",
      "        DropPath-298            [-1, 304, 3, 3]               0\n",
      "SDTAEncoder_KANLinear-299            [-1, 304, 3, 3]               0\n",
      "          Conv2d-300             [-1, 61, 3, 3]             610\n",
      "          Conv2d-301             [-1, 61, 3, 3]             610\n",
      "          Conv2d-302             [-1, 61, 3, 3]             610\n",
      "          Conv2d-303             [-1, 61, 3, 3]             610\n",
      "       LayerNorm-304               [-1, 9, 304]             608\n",
      "            SiLU-305                  [-1, 304]               0\n",
      "       KANLinear-306               [-1, 9, 152]               0\n",
      "            SiLU-307                  [-1, 152]               0\n",
      "       KANLinear-308               [-1, 9, 912]               0\n",
      "     LoRaLin_KAN-309               [-1, 9, 912]               0\n",
      "         Dropout-310            [-1, 8, 38, 38]               0\n",
      "            SiLU-311                  [-1, 304]               0\n",
      "       KANLinear-312               [-1, 9, 152]               0\n",
      "            SiLU-313                  [-1, 152]               0\n",
      "       KANLinear-314               [-1, 9, 304]               0\n",
      "     LoRaLin_KAN-315               [-1, 9, 304]               0\n",
      "         Dropout-316               [-1, 9, 304]               0\n",
      "   XCA_KANLinear-317               [-1, 9, 304]               0\n",
      "        DropPath-318               [-1, 9, 304]               0\n",
      "       LayerNorm-319            [-1, 3, 3, 304]             608\n",
      "            SiLU-320                  [-1, 304]               0\n",
      "       KANLinear-321            [-1, 3, 3, 152]               0\n",
      "            SiLU-322                  [-1, 152]               0\n",
      "       KANLinear-323           [-1, 3, 3, 1216]               0\n",
      "     LoRaLin_KAN-324           [-1, 3, 3, 1216]               0\n",
      "            GELU-325           [-1, 3, 3, 1216]               0\n",
      "            SiLU-326                 [-1, 1216]               0\n",
      "       KANLinear-327            [-1, 3, 3, 152]               0\n",
      "            SiLU-328                  [-1, 152]               0\n",
      "       KANLinear-329            [-1, 3, 3, 304]               0\n",
      "     LoRaLin_KAN-330            [-1, 3, 3, 304]               0\n",
      "        DropPath-331            [-1, 304, 3, 3]               0\n",
      "SDTAEncoder_KANLinear-332            [-1, 304, 3, 3]               0\n",
      "       LayerNorm-333                  [-1, 304]             608\n",
      "         Dropout-334                  [-1, 304]               0\n",
      "            SiLU-335                  [-1, 304]               0\n",
      "       KANLinear-336                  [-1, 152]               0\n",
      "            SiLU-337                  [-1, 152]               0\n",
      "       KANLinear-338                  [-1, 512]               0\n",
      "     LoRaLin_KAN-339                  [-1, 512]               0\n",
      "================================================================\n",
      "Total params: 386,968\n",
      "Trainable params: 386,968\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 38.49\n",
      "Params size (MB): 1.48\n",
      "Estimated Total Size (MB): 40.11\n",
      "----------------------------------------------------------------\n",
      "Total parameters: 37058848\n"
     ]
    }
   ],
   "source": [
    "# Summary of the model\n",
    "summary(model, (3, 112, 112), device=\"cpu\")  # Use \"cuda\" if a GPU is available\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhomnhom0/miniforge3/envs/face_recognition/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EdgeFace_KANConv(\n",
      "  (downsample_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): KAN_Convolutional_Layer(\n",
      "        (convs): ModuleList(\n",
      "          (0-143): 144 x KAN_Convolution(\n",
      "            (conv): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): LayerNorm()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): LayerNorm()\n",
      "      (1): KAN_Convolutional_Layer(\n",
      "        (convs): ModuleList(\n",
      "          (0-4607): 4608 x KAN_Convolution(\n",
      "            (conv): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): LayerNorm()\n",
      "      (1): KAN_Convolutional_Layer(\n",
      "        (convs): ModuleList(\n",
      "          (0-15359): 15360 x KAN_Convolution(\n",
      "            (conv): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): LayerNorm()\n",
      "      (1): KAN_Convolutional_Layer(\n",
      "        (convs): ModuleList(\n",
      "          (0-48639): 48640 x KAN_Convolution(\n",
      "            (conv): KANLinear(\n",
      "              (base_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stages): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-2303): 2304 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=48, out_features=9, bias=False)\n",
      "          (linear2): Linear(in_features=9, out_features=192, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=192, out_features=9, bias=False)\n",
      "          (linear2): Linear(in_features=9, out_features=48, bias=True)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-2303): 2304 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=48, out_features=9, bias=False)\n",
      "          (linear2): Linear(in_features=9, out_features=192, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=192, out_features=9, bias=False)\n",
      "          (linear2): Linear(in_features=9, out_features=48, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.006)\n",
      "      )\n",
      "      (2): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-2303): 2304 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=48, out_features=9, bias=False)\n",
      "          (linear2): Linear(in_features=9, out_features=192, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=192, out_features=9, bias=False)\n",
      "          (linear2): Linear(in_features=9, out_features=48, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.012)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-9215): 9216 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=96, out_features=19, bias=False)\n",
      "          (linear2): Linear(in_features=19, out_features=384, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=384, out_features=19, bias=False)\n",
      "          (linear2): Linear(in_features=19, out_features=96, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.018)\n",
      "      )\n",
      "      (1): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-9215): 9216 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=96, out_features=19, bias=False)\n",
      "          (linear2): Linear(in_features=19, out_features=384, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=384, out_features=19, bias=False)\n",
      "          (linear2): Linear(in_features=19, out_features=96, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.024)\n",
      "      )\n",
      "      (2): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-9215): 9216 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=96, out_features=19, bias=False)\n",
      "          (linear2): Linear(in_features=19, out_features=384, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=384, out_features=19, bias=False)\n",
      "          (linear2): Linear(in_features=19, out_features=96, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.029)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.035)\n",
      "      )\n",
      "      (1): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.041)\n",
      "      )\n",
      "      (2): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.047)\n",
      "      )\n",
      "      (3): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.053)\n",
      "      )\n",
      "      (4): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.059)\n",
      "      )\n",
      "      (5): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.065)\n",
      "      )\n",
      "      (6): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.071)\n",
      "      )\n",
      "      (7): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.076)\n",
      "      )\n",
      "      (8): ConvEncoder_KANConv(\n",
      "        (dwconv): KAN_Convolutional_Layer(\n",
      "          (convs): ModuleList(\n",
      "            (0-25599): 25600 x KAN_Convolution(\n",
      "              (conv): KANLinear(\n",
      "                (base_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=160, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=640, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=640, out_features=32, bias=False)\n",
      "          (linear2): Linear(in_features=32, out_features=160, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.082)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): SDTAEncoder_KANConv(\n",
      "        (convs): ModuleList(\n",
      "          (0-3): 4 x KAN_Convolutional_Layer(\n",
      "            (convs): ModuleList(\n",
      "              (0-3720): 3721 x KAN_Convolution(\n",
      "                (conv): KANLinear(\n",
      "                  (base_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm_xca): LayerNorm()\n",
      "        (xca): XCA(\n",
      "          (qkv): LoRaLin(\n",
      "            (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "            (linear2): Linear(in_features=60, out_features=912, bias=True)\n",
      "          )\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): LoRaLin(\n",
      "            (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "            (linear2): Linear(in_features=60, out_features=304, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "          (linear2): Linear(in_features=60, out_features=1216, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=1216, out_features=60, bias=False)\n",
      "          (linear2): Linear(in_features=60, out_features=304, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.088)\n",
      "      )\n",
      "      (1): SDTAEncoder_KANConv(\n",
      "        (convs): ModuleList(\n",
      "          (0-3): 4 x KAN_Convolutional_Layer(\n",
      "            (convs): ModuleList(\n",
      "              (0-3720): 3721 x KAN_Convolution(\n",
      "                (conv): KANLinear(\n",
      "                  (base_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm_xca): LayerNorm()\n",
      "        (xca): XCA(\n",
      "          (qkv): LoRaLin(\n",
      "            (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "            (linear2): Linear(in_features=60, out_features=912, bias=True)\n",
      "          )\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): LoRaLin(\n",
      "            (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "            (linear2): Linear(in_features=60, out_features=304, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "          (linear2): Linear(in_features=60, out_features=1216, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=1216, out_features=60, bias=False)\n",
      "          (linear2): Linear(in_features=60, out_features=304, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.094)\n",
      "      )\n",
      "      (2): SDTAEncoder_KANConv(\n",
      "        (convs): ModuleList(\n",
      "          (0-3): 4 x KAN_Convolutional_Layer(\n",
      "            (convs): ModuleList(\n",
      "              (0-3720): 3721 x KAN_Convolution(\n",
      "                (conv): KANLinear(\n",
      "                  (base_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm_xca): LayerNorm()\n",
      "        (xca): XCA(\n",
      "          (qkv): LoRaLin(\n",
      "            (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "            (linear2): Linear(in_features=60, out_features=912, bias=True)\n",
      "          )\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): LoRaLin(\n",
      "            (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "            (linear2): Linear(in_features=60, out_features=304, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (pwconv1): LoRaLin(\n",
      "          (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "          (linear2): Linear(in_features=60, out_features=1216, bias=True)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (pwconv2): LoRaLin(\n",
      "          (linear1): Linear(in_features=1216, out_features=60, bias=False)\n",
      "          (linear2): Linear(in_features=60, out_features=304, bias=True)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.100)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((304,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): LoRaLin(\n",
      "    (linear1): Linear(in_features=304, out_features=60, bias=False)\n",
      "    (linear2): Linear(in_features=60, out_features=512, bias=True)\n",
      "  )\n",
      "  (head_dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "from models.KANConv_EdgeFace import EdgeFace_KANConv\n",
    "\n",
    "# Define the model\n",
    "model = EdgeFace_KANConv(\n",
    "    in_chans=3, \n",
    "    num_classes=512,\n",
    "    depths=[3, 3, 9, 3],\n",
    "    dims=[48, 96, 160, 304],\n",
    "    global_block=[0, 0, 0, 3],\n",
    "    global_block_type=['None', 'None', 'None', 'SDTA'],\n",
    "    drop_path_rate=0.1, \n",
    "    layer_scale_init_value=1e-6, \n",
    "    head_init_scale=1.0, \n",
    "    expan_ratio=4,\n",
    "    kernel_sizes=[(7, 7), (7, 7), (7, 7), (7, 7)],\n",
    "    heads=[8, 8, 8, 8],\n",
    "    use_pos_embd_xca=[False, False, False, False],\n",
    "    use_pos_embd_global=False,\n",
    "    d2_scales=[2, 3, 4, 5],\n",
    "    classifier_dropout=0.5  # Add any additional kwargs\n",
    ")\n",
    "\n",
    "# Print the model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random tensor of shape (batch_size, 3, 112, 112) to pass through the model\n",
    "input_tensor = torch.randn(1, 3, 112, 112)  # Batch size of 1\n",
    "\n",
    "# Pass the tensor through the model\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Print the output tensor shape\n",
    "print(f\"Output tensor shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
